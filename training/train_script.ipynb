{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab3c7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"/content/lora_adapter\"\n",
    "DATA_FILE = \"/content/cli_qa.json\"\n",
    "LOG_DIR = \"/content/logs\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üöÄ Starting TinyLlama fine-tuning...\")\n",
    "print(f\"üìÖ Start time: {datetime.now()}\")\n",
    "print(f\"üî• CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"\\nüìä Loading Q&A data...\")\n",
    "try:\n",
    "    with open(DATA_FILE, 'r') as f:\n",
    "        qa_data = json.load(f)\n",
    "    print(f\"üìà Successfully loaded {len(qa_data)} Q&A pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading data: {e}\")\n",
    "    print(\"üí° Please upload your cli_qa.json file to Colab first\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if 'cli_qa.json' in uploaded:\n",
    "        with open(DATA_FILE, 'wb') as f:\n",
    "            f.write(uploaded['cli_qa.json'])\n",
    "        print(\"‚úÖ File uploaded successfully!\")\n",
    "        with open(DATA_FILE, 'r') as f:\n",
    "            qa_data = json.load(f)\n",
    "    else:\n",
    "        print(\"‚ùå No file uploaded. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "# Step 2: Format data for training\n",
    "def format_prompt(question, answer):\n",
    "    \"\"\"Format Q&A pair as a chat prompt for TinyLlama\"\"\"\n",
    "    return f\"<|system|>\\nYou are a helpful command-line assistant.\\n<|user|>\\n{question}\\n<|assistant|>\\n{answer}<|end|>\"\n",
    "\n",
    "formatted_data = []\n",
    "for item in qa_data:\n",
    "    formatted_text = format_prompt(item['question'], item['answer'])\n",
    "    formatted_data.append({\"text\": formatted_text})\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_data)} training examples\")\n",
    "\n",
    "# Step 3: Load tokenizer and model\n",
    "print(\"\\nü§ñ Loading TinyLlama model and tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    print(\"‚úÖ Tokenizer loaded and configured\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(f\"‚úÖ Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading model: {e}\")\n",
    "    print(\"üí° Try running: !pip install --upgrade transformers torch peft\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 4: Configure LoRA\n",
    "print(\"\\n‚öô Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/model.num_parameters()*100:.2f}%)\")\n",
    "\n",
    "# Step 5: Prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data\"\"\"\n",
    "    texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
    "\n",
    "    outputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    outputs[\"labels\"] = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        labels = input_ids.copy()\n",
    "        labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        outputs[\"labels\"].append(labels)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "print(\"üîÑ Creating and tokenizing dataset...\")\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Debug info\n",
    "lengths = [len(sample['input_ids']) for sample in tokenized_dataset]\n",
    "print(f\"\\nüî¢ Dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"üìè All input lengths: {list(set(lengths))}\")  # Convert to list for display\n",
    "\n",
    "# Step 6: Training configuration\n",
    "print(\"\\nüèã Setting up training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=LOG_DIR,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Step 7: Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 8: Start training\n",
    "print(\"\\nüî• Starting training...\")\n",
    "start_time = datetime.now()\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "print(f\"‚è± Training duration: {training_duration}\")\n",
    "\n",
    "# Step 9: Save the model\n",
    "print(\"\\nüíæ Saving LoRA adapter...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Optional: Save to Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_output_dir = \"/content/drive/MyDrive/lora_adapter\"\n",
    "    os.makedirs(drive_output_dir, exist_ok=True)\n",
    "    model.save_pretrained(drive_output_dir)\n",
    "    tokenizer.save_pretrained(drive_output_dir)\n",
    "    print(f\"üíæ Also saved to Google Drive at: {drive_output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Couldn't save to Google Drive: {e}\")\n",
    "\n",
    "# Step 10: Create training log (FIXED VERSION)\n",
    "unique_lengths = list(set(lengths))  # Convert set to list properly\n",
    "\n",
    "log_data = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_start\": start_time.isoformat(),\n",
    "    \"training_end\": end_time.isoformat(),\n",
    "    \"training_duration\": str(training_duration),\n",
    "    \"num_examples\": len(formatted_data),\n",
    "    \"num_epochs\": 1,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"input_lengths\": unique_lengths,  # Use the pre-converted list\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"target_modules\": list(lora_config.target_modules)  # Convert tuple to list\n",
    "    }\n",
    "}\n",
    "\n",
    "log_path = os.path.join(LOG_DIR, \"training_log.json\")\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(log_data, f, indent=2)\n",
    "print(f\"\\nüìù Training log saved to: {log_path}\")\n",
    "\n",
    "# Step 11: Test the model\n",
    "print(\"\\nüß™ Quick test of fine-tuned model...\")\n",
    "test_prompt = \"How do I list all files in a directory?\"\n",
    "input_text = f\"<|system|>\\nYou are a helpful command-line assistant.\\n<|user|>\\n{test_prompt}\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Test response:\\n{response}\")\n",
    "\n",
    "print(\"\\n‚ú® All done! Ready for Phase 3.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
