{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c342556",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"./lora_adapter\"\n",
    "DATA_FILE = \"./data/cli_qa.json\"\n",
    "LOG_DIR = \"./logs\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üöÄ Starting TinyLlama fine-tuning...\")\n",
    "print(f\"üìÖ Start time: {datetime.now()}\")\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "print(\"\\nüìä Loading Q&A data...\")\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "print(f\"üìà Loaded {len(qa_data)} Q&A pairs\")\n",
    "\n",
    "# Step 2: Format data for training\n",
    "def format_prompt(question, answer):\n",
    "    \"\"\"Format Q&A pair as a chat prompt for TinyLlama\"\"\"\n",
    "    return f\"<|system|>\\nYou are a helpful command-line assistant.\\n<|user|>\\n{question}\\n<|assistant|>\\n{answer}<|end|>\"\n",
    "\n",
    "formatted_data = []\n",
    "for item in qa_data:\n",
    "    formatted_text = format_prompt(item['question'], item['answer'])\n",
    "    formatted_data.append({\"text\": formatted_text})\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_data)} training examples\")\n",
    "\n",
    "# Step 3: Load tokenizer and model\n",
    "print(\"\\nü§ñ Loading TinyLlama model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"üìä Model loaded. Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Step 4: Configure LoRA\n",
    "print(\"\\n‚öô Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,                    # Low rank\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    lora_dropout=0.1,       # Dropout\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Target attention layers\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/model.num_parameters()*100:.2f}%)\")\n",
    "\n",
    "# Step 5: Prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data\"\"\"\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"üî¢ Dataset size: {len(tokenized_dataset)}\")\n",
    "\n",
    "# Step 6: Training configuration\n",
    "print(\"\\nüèã Setting up training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,              # 1 epoch as required\n",
    "    per_device_train_batch_size=4,   # Small batch for memory efficiency\n",
    "    gradient_accumulation_steps=4,   # Effective batch size = 16\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,                       # Mixed precision\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    logging_dir=LOG_DIR,\n",
    "    report_to=None,                  # Disable wandb\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Step 7: Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 8: Start training\n",
    "print(\"\\nüî• Starting training...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "print(f\"‚è± Training duration: {training_duration}\")\n",
    "\n",
    "# Step 9: Save the model\n",
    "print(\"\\nüíæ Saving LoRA adapter...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Step 10: Create training log\n",
    "log_data = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_start\": start_time.isoformat(),\n",
    "    \"training_end\": end_time.isoformat(),\n",
    "    \"training_duration\": str(training_duration),\n",
    "    \"num_examples\": len(formatted_data),\n",
    "    \"num_epochs\": 1,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"target_modules\": lora_config.target_modules\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(LOG_DIR, \"training_log.json\"), 'w') as f:\n",
    "    json.dump(log_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüéâ Fine-tuning complete!\")\n",
    "print(f\"üìÅ LoRA adapter saved to: {OUTPUT_DIR}\")\n",
    "print(f\"üìù Training log saved to: {LOG_DIR}/training_log.json\")\n",
    "\n",
    "# Step 11: Test the model quickly\n",
    "print(\"\\nüß™ Quick test of fine-tuned model...\")\n",
    "test_prompt = \"How do I list all files in a directory?\"\n",
    "inputs = tokenizer.encode(f\"<|system|>\\nYou are a helpful command-line assistant.\\n<|user|>\\n{test_prompt}\\n<|assistant|>\\n\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=100, \n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Test response:\\n{response}\")\n",
    "\n",
    "print(\"\\n‚ú® All done! Ready for Phase 3.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
